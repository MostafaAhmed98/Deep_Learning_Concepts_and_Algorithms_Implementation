{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Vanishing Gradients:**\n",
    "    - **Definition:** In some cases, when a neural network is deep, the gradients (indicators used to adjust the model's parameters during training) can become extremely small as they are propagated backward through the network.\n",
    "    - **Key Aspect:** This can lead to the early layers of the network learning very slowly or not learning at all, as the updates to the parameters become too tiny to make a significant impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights after training (vanishing gradients):\n",
      "[[ 0.03920641 -0.00065686  0.00055813]\n",
      " [-0.02343697  0.06295061  0.03688675]\n",
      " [ 0.01939752 -0.03821543  0.05544233]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a simple neural network with small weights\n",
    "def vanishing_gradients_example():\n",
    "    np.random.seed(42)\n",
    "    input_data = np.random.rand(1, 3)  # Input data with three features\n",
    "    weights = np.random.rand(3, 3) * 0.1  # Small random weights\n",
    "\n",
    "    for _ in range(10):\n",
    "        # Forward pass\n",
    "        hidden_layer = np.dot(input_data, weights)\n",
    "        output = np.dot(hidden_layer, weights)\n",
    "\n",
    "        # Backward pass (Gradient computation)\n",
    "        output_gradient = np.random.rand(1, 3)  # Gradient of the output layer (random for illustration)\n",
    "        hidden_layer_gradient = np.dot(output_gradient, weights.T)  # Gradient at the hidden layer\n",
    "\n",
    "        # Update weights (Gradient Descent)\n",
    "        learning_rate = 0.1\n",
    "        weights -= learning_rate * hidden_layer.T.dot(output_gradient)\n",
    "    print(\"Updated weights after training (vanishing gradients):\")\n",
    "    print(weights)\n",
    "\n",
    "vanishing_gradients_example()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Exploding Gradients:**\n",
    "    - **Definition:** Conversely, exploding gradients occur when the gradients become exceptionally large during backpropagation.\n",
    "    - **Key Aspect:** This can lead to unstable training, where the model's parameters are adjusted by a large extent, causing the model to diverge or fail to converge to a good solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights after training (exploding gradients):\n",
      "[[ 3.92064111 -0.06568602  0.0558127 ]\n",
      " [-2.34369726  6.29506135  3.68867495]\n",
      " [ 1.9397522  -3.82154291  5.5442327 ]]\n"
     ]
    }
   ],
   "source": [
    "# Define a simple neural network with large weights\n",
    "def exploding_gradients_example():\n",
    "    np.random.seed(42)\n",
    "    input_data = np.random.rand(1, 3)  # Input data with three features\n",
    "    weights = np.random.rand(3, 3) * 10  # Large random weights\n",
    "\n",
    "    for _ in range(10):\n",
    "        # Forward pass\n",
    "        hidden_layer = np.dot(input_data, weights)\n",
    "        output = np.dot(hidden_layer, weights)\n",
    "\n",
    "        # Backward pass (Gradient computation)\n",
    "        output_gradient = np.random.rand(1, 3)  # Gradient of the output layer (random for illustration)\n",
    "        hidden_layer_gradient = np.dot(output_gradient, weights.T)  # Gradient at the hidden layer\n",
    "\n",
    "        # Update weights (Gradient Descent)\n",
    "        learning_rate = 0.1\n",
    "        weights -= learning_rate * hidden_layer.T.dot(output_gradient)\n",
    "\n",
    "    print(\"Updated weights after training (exploding gradients):\")\n",
    "    print(weights)\n",
    "\n",
    "exploding_gradients_example()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Consequences:**\n",
    "    - **Vanishing Gradients Consequence:** Layers that are too deep might not effectively learn meaningful representations, impacting the overall performance of the model.\n",
    "    - **Exploding Gradients Consequence:** Unstable training can hinder convergence, making it difficult for the model to learn useful patterns from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Mitigation Strategies:**\n",
    "    - **Weight Initialization:** Careful initialization of the weights can help alleviate both vanishing and exploding gradients.\n",
    "    - **Activation Functions:** Choosing appropriate activation functions, such as ReLU, can mitigate vanishing gradient problems.\n",
    "    - **Batch Normalization:** Normalizing intermediate layer outputs helps stabilize training by reducing internal covariate shift.\n",
    "    - **Gradient Clipping:** Setting a threshold for the gradient values can prevent them from becoming too large during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xavier Initialization (Glorot Initialization):\n",
    "\n",
    "It aims to keep the variance of activations and gradients roughly the same across layers.\n",
    "It's well-suited for tanh and sigmoid activation functions.\n",
    "The weights are randomly drawn from a distribution with a standard deviation based on the number of input and output units in a layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights after training (Xavier initialization):\n",
      "[[ 0.24504301 -0.29374122 -0.2895481 ]\n",
      " [-0.43516865  0.48203275  0.17840939]\n",
      " [ 0.06683412 -0.69038429  0.40046285]]\n"
     ]
    }
   ],
   "source": [
    "# Define a simple neural network with large weights\n",
    "def Xavier_init(input, output):\n",
    "    gain = np.sqrt(2.0 / (input + output))\n",
    "    return np.random.uniform(-gain, gain, size=(input, output))\n",
    "\n",
    "def Xavier_init_example():\n",
    "    np.random.seed(42)\n",
    "    input_data = np.random.rand(1, 3)  # Input data with three features\n",
    "    weights = Xavier_init(3, 3) # Xavier\n",
    "\n",
    "    for _ in range(10):\n",
    "        # Forward pass\n",
    "        hidden_layer = np.dot(input_data, weights) # 1x3 * 3x3 = 1x3\n",
    "        output = np.dot(hidden_layer, weights) # 1x3 * 3x3 = 1x3\n",
    "\n",
    "        # Backward pass (Gradient computation)\n",
    "        output_gradient = np.random.rand(1, 3)  # Gradient of the output layer (random for illustration) \n",
    "        hidden_layer_gradient = np.dot(output_gradient, weights.T)  # Gradient at the hidden layer\n",
    "\n",
    "        # Update weights (Gradient Descent)\n",
    "        learning_rate = 0.1\n",
    "        weights -= learning_rate * hidden_layer.T.dot(output_gradient)\n",
    "\n",
    "    print(\"Updated weights after training (Xavier initialization):\")\n",
    "    print(weights)\n",
    "\n",
    "Xavier_init_example()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## He Initialization:\n",
    "\n",
    "It's a variant of Xavier initialization designed for ReLU activation functions.\n",
    "It uses a different scaling factor to account for the \"dying ReLU\" problem, where neurons get stuck at zero activation.\n",
    "\n",
    "<b>The \"Dying ReLU\" Problem:</b>\n",
    "\n",
    "Vanilla ReLU activations have a binary nature: they output 0 for negative inputs and the original value for positive inputs. This can lead to the \"dying ReLU\" problem, where some neurons get stuck in a permanently inactive state due to receiving mostly negative inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights after training (He initialization):\n",
      "[[-1.11077427  0.06486551  0.00779027]\n",
      " [ 1.0969696  -0.23179696 -0.15457116]\n",
      " [ 0.22889471 -0.14776024 -1.42075096]]\n"
     ]
    }
   ],
   "source": [
    "# Define a simple neural network with large weights\n",
    "def he_init(input, output):\n",
    "    gain = np.sqrt(2.0 / input )\n",
    "    return np.random.randn(input, output) * gain\n",
    "\n",
    "def he_init_example():\n",
    "    np.random.seed(42)\n",
    "    input_data = np.random.rand(1, 3)  # Input data with three features\n",
    "    weights = he_init(3, 3) # He initialization\n",
    "\n",
    "    for _ in range(10):\n",
    "        # Forward pass\n",
    "        hidden_layer = np.dot(input_data, weights) # 1x3 * 3x3 = 1x3\n",
    "        output = np.dot(hidden_layer, weights) # 1x3 * 3x3 = 1x3\n",
    "\n",
    "        # Backward pass (Gradient computation)\n",
    "        output_gradient = np.random.rand(1, 3)  # Gradient of the output layer (random for illustration) \n",
    "        hidden_layer_gradient = np.dot(output_gradient, weights.T)  # Gradient at the hidden layer\n",
    "\n",
    "        # Update weights (Gradient Descent)\n",
    "        learning_rate = 0.1\n",
    "        weights -= learning_rate * hidden_layer.T.dot(output_gradient)\n",
    "\n",
    "    print(\"Updated weights after training (He initialization):\")\n",
    "    print(weights)\n",
    "\n",
    "he_init_example()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpuenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
